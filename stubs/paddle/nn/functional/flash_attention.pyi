from _typeshed import Incomplete
from collections.abc import Generator
from paddle import in_dynamic_mode as in_dynamic_mode
from paddle.base.layer_helper import LayerHelper as LayerHelper
from paddle.base.wrapped_decorator import signature_safe_contextmanager as signature_safe_contextmanager

g_enable_math: Incomplete
g_enable_flash: Incomplete
g_enable_mem_efficient: Incomplete

def sdp_kernel(enable_math: bool = False, enable_flash: bool = True, enable_mem_efficient: bool = True) -> Generator[None]: ...
def get_triangle_upper_mask(x): ...
def flash_attention(query, key, value, dropout: float = 0.0, causal: bool = False, return_softmax: bool = False, *, fixed_seed_offset: Incomplete | None = None, rng_name: str = '', training: bool = True, name: Incomplete | None = None): ...
def flash_attn_unpadded(query, key, value, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, scale, dropout: float = 0.0, causal: bool = False, return_softmax: bool = False, fixed_seed_offset: Incomplete | None = None, rng_name: str = '', training: bool = True, name: Incomplete | None = None): ...
def scaled_dot_product_attention(query, key, value, attn_mask: Incomplete | None = None, dropout_p: float = 0.0, is_causal: bool = False, training: bool = True, name: Incomplete | None = None): ...
