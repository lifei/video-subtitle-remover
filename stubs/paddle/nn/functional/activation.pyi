from ...base.data_feeder import check_dtype as check_dtype, check_variable_and_dtype as check_variable_and_dtype
from ...base.framework import convert_np_dtype_to_dtype_ as convert_np_dtype_to_dtype_
from ...base.layer_helper import LayerHelper as LayerHelper
from ...tensor.manipulation import chunk as chunk
from ...tensor.math import tanh as tanh, tanh_ as tanh_
from ...tensor.ops import sigmoid as sigmoid
from _typeshed import Incomplete
from paddle import in_dynamic_mode as in_dynamic_mode
from paddle.framework import core as core, in_dynamic_or_pir_mode as in_dynamic_or_pir_mode
from paddle.utils.inplace_utils import inplace_apis_in_dygraph_only as inplace_apis_in_dygraph_only

def celu(x, alpha: float = 1.0, name: Incomplete | None = None): ...
def elu(x, alpha: float = 1.0, name: Incomplete | None = None): ...
def elu_(x, alpha: float = 1.0, name: Incomplete | None = None): ...
def gelu(x, approximate: bool = False, name: Incomplete | None = None): ...
def hardshrink(x, threshold: float = 0.5, name: Incomplete | None = None): ...
def hardtanh(x, min: float = -1.0, max: float = 1.0, name: Incomplete | None = None): ...
def hardtanh_(x, min: float = -1.0, max: float = 1.0, name: Incomplete | None = None): ...
def hardsigmoid(x, slope: float = 0.1666667, offset: float = 0.5, name: Incomplete | None = None): ...
def hardswish(x, name: Incomplete | None = None): ...
def leaky_relu(x, negative_slope: float = 0.01, name: Incomplete | None = None): ...
def leaky_relu_(x, negative_slope: float = 0.01, name: Incomplete | None = None): ...
def prelu(x, weight, data_format: str = 'NCHW', name: Incomplete | None = None): ...
def rrelu(x, lower=..., upper=..., training: bool = True, name: Incomplete | None = None): ...
def relu(x, name: Incomplete | None = None): ...
def relu_(x, name: Incomplete | None = None): ...
def log_sigmoid(x, name: Incomplete | None = None): ...
def maxout(x, groups, axis: int = 1, name: Incomplete | None = None): ...
def relu6(x, name: Incomplete | None = None): ...
def selu(x, scale: float = 1.0507009873554805, alpha: float = 1.6732632423543772, name: Incomplete | None = None): ...
def silu(x, name: Incomplete | None = None): ...
def softmax(x, axis: int = -1, dtype: Incomplete | None = None, name: Incomplete | None = None): ...
def softmax_(x, axis: int = -1, dtype: Incomplete | None = None, name: Incomplete | None = None): ...
def softplus(x, beta: int = 1, threshold: int = 20, name: Incomplete | None = None): ...
def softshrink(x, threshold: float = 0.5, name: Incomplete | None = None): ...
def softsign(x, name: Incomplete | None = None): ...
def swish(x, name: Incomplete | None = None): ...
def mish(x, name: Incomplete | None = None): ...
def tanhshrink(x, name: Incomplete | None = None): ...
def thresholded_relu(x, threshold: float = 1.0, name: Incomplete | None = None): ...
def thresholded_relu_(x, threshold: float = 1.0, name: Incomplete | None = None): ...
def log_softmax(x, axis: int = -1, dtype: Incomplete | None = None, name: Incomplete | None = None): ...
def glu(x, axis: int = -1, name: Incomplete | None = None): ...
def gumbel_softmax(x, temperature: float = 1.0, hard: bool = False, axis: int = -1, name: Incomplete | None = None): ...
