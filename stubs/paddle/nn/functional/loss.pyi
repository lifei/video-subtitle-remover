from ...base.data_feeder import check_variable_and_dtype as check_variable_and_dtype
from ...base.framework import core as core, in_dynamic_or_pir_mode as in_dynamic_or_pir_mode, in_pir_mode as in_pir_mode
from ...base.layer_helper import LayerHelper as LayerHelper
from ...common_ops_import import Variable as Variable
from ...tensor.manipulation import reshape as reshape
from _typeshed import Incomplete
from paddle import base as base, in_dynamic_mode as in_dynamic_mode
from paddle.static.nn.control_flow import Assert as Assert
from paddle.utils import deprecated as deprecated

kIgnoreIndex: int

def dice_loss(input, label, epsilon: float = 1e-05, name: Incomplete | None = None): ...
def log_loss(input, label, epsilon: float = 0.0001, name: Incomplete | None = None): ...
def base_softmax_with_cross_entropy(logits, label, soft_label: bool = False, ignore_index: int = -100, numeric_stable_mode: bool = True, return_softmax: bool = False, axis: int = -1): ...
def npair_loss(anchor, positive, labels, l2_reg: float = 0.002): ...
def square_error_cost(input, label): ...
def edit_distance(input, label, normalized: bool = True, ignored_tokens: Incomplete | None = None, input_length: Incomplete | None = None, label_length: Incomplete | None = None): ...
def binary_cross_entropy(input, label, weight: Incomplete | None = None, reduction: str = 'mean', name: Incomplete | None = None): ...
def binary_cross_entropy_with_logits(logit, label, weight: Incomplete | None = None, reduction: str = 'mean', pos_weight: Incomplete | None = None, name: Incomplete | None = None): ...
def hsigmoid_loss(input, label, num_classes, weight, bias: Incomplete | None = None, path_table: Incomplete | None = None, path_code: Incomplete | None = None, is_sparse: bool = False, name: Incomplete | None = None): ...
def smooth_l1_loss(input, label, reduction: str = 'mean', delta: float = 1.0, name: Incomplete | None = None): ...
def margin_ranking_loss(input, other, label, margin: float = 0.0, reduction: str = 'mean', name: Incomplete | None = None): ...
def l1_loss(input, label, reduction: str = 'mean', name: Incomplete | None = None): ...
def nll_loss(input, label, weight: Incomplete | None = None, ignore_index: int = -100, reduction: str = 'mean', name: Incomplete | None = None): ...
def poisson_nll_loss(input, label, log_input: bool = True, full: bool = False, epsilon: float = 1e-08, reduction: str = 'mean', name: Incomplete | None = None): ...
def kl_div(input, label, reduction: str = 'mean', name: Incomplete | None = None): ...
def mse_loss(input, label, reduction: str = 'mean', name: Incomplete | None = None): ...
def ctc_loss(log_probs, labels, input_lengths, label_lengths, blank: int = 0, reduction: str = 'mean', norm_by_times: bool = False): ...
def rnnt_loss(input, label, input_lengths, label_lengths, blank: int = 0, fastemit_lambda: float = 0.001, reduction: str = 'mean', name: Incomplete | None = None): ...
def margin_cross_entropy(logits, label, margin1: float = 1.0, margin2: float = 0.5, margin3: float = 0.0, scale: float = 64.0, group: Incomplete | None = None, return_softmax: bool = False, reduction: str = 'mean'): ...
def softmax_with_cross_entropy(logits, label, soft_label: bool = False, ignore_index: int = -100, numeric_stable_mode: bool = True, return_softmax: bool = False, axis: int = -1): ...
def cross_entropy(input, label, weight: Incomplete | None = None, ignore_index: int = -100, reduction: str = 'mean', soft_label: bool = False, axis: int = -1, use_softmax: bool = True, label_smoothing: float = 0.0, name: Incomplete | None = None): ...
def sigmoid_focal_loss(logit, label, normalizer: Incomplete | None = None, alpha: float = 0.25, gamma: float = 2.0, reduction: str = 'sum', name: Incomplete | None = None): ...
def multi_label_soft_margin_loss(input, label, weight: Incomplete | None = None, reduction: str = 'mean', name: Incomplete | None = None): ...
def hinge_embedding_loss(input, label, margin: float = 1.0, reduction: str = 'mean', name: Incomplete | None = None): ...
def cosine_embedding_loss(input1, input2, label, margin: int = 0, reduction: str = 'mean', name: Incomplete | None = None): ...
def triplet_margin_with_distance_loss(input, positive, negative, distance_function: Incomplete | None = None, margin: float = 1.0, swap: bool = False, reduction: str = 'mean', name: Incomplete | None = None): ...
def triplet_margin_loss(input, positive, negative, margin: float = 1.0, p: int = 2, epsilon: float = 1e-06, swap: bool = False, reduction: str = 'mean', name: Incomplete | None = None): ...
def multi_margin_loss(input, label, p: int = 1, margin: float = 1.0, weight: Incomplete | None = None, reduction: str = 'mean', name: Incomplete | None = None): ...
def soft_margin_loss(input, label, reduction: str = 'mean', name: Incomplete | None = None): ...
def gaussian_nll_loss(input, label, variance, full: bool = False, epsilon: float = 1e-06, reduction: str = 'mean', name: Incomplete | None = None): ...
