from ..layer.layers import Layer
from _typeshed import Incomplete

__all__ = ['FakeQuantAbsMax', 'FakeQuantMovingAverageAbsMax', 'FakeQuantChannelWiseAbsMax', 'QuantizedConv2D', 'QuantizedConv2DTranspose', 'QuantizedLinear', 'MovingAverageAbsMaxScale', 'MAOutputScaleLayer', 'FakeQuantMAOutputScaleLayer', 'QuantStub', 'QuantizedRowParallelLinear', 'QuantizedColumnParallelLinear', 'QuantizedMatmul']

class FakeQuantAbsMax(Layer):
    def __init__(self, name: Incomplete | None = None, quant_bits: int = 8, dtype: str = 'float32', quant_on_weight: bool = False, reduce_type: Incomplete | None = None) -> None: ...
    def forward(self, input): ...

class FakeQuantMovingAverageAbsMax(Layer):
    def __init__(self, name: Incomplete | None = None, moving_rate: float = 0.9, quant_bits: int = 8, dtype: str = 'float32', reduce_type: Incomplete | None = None) -> None: ...
    def forward(self, input): ...

class FakeQuantChannelWiseAbsMax(Layer):
    def __init__(self, name: Incomplete | None = None, channel_num: Incomplete | None = None, quant_bits: int = 8, quant_axis: int = 0, dtype: str = 'float32', quant_on_weight: bool = False, reduce_type: Incomplete | None = None) -> None: ...
    def forward(self, input): ...

class MovingAverageAbsMaxScale(Layer):
    def __init__(self, name: Incomplete | None = None, moving_rate: float = 0.9, dtype: str = 'float32', reduce_type: Incomplete | None = None) -> None: ...
    def forward(self, input): ...
QuantStub = MovingAverageAbsMaxScale

class QuantizedConv2D(Layer):
    weight: Incomplete
    bias: Incomplete
    def __init__(self, layer, weight_bits: int = 8, activation_bits: int = 8, moving_rate: float = 0.9, weight_quantize_type: str = 'abs_max', activation_quantize_type: str = 'abs_max', weight_pre_layer: Incomplete | None = None, act_pre_layer: Incomplete | None = None, weight_quant_layer: Incomplete | None = None, act_quant_layer: Incomplete | None = None) -> None: ...
    def forward(self, input): ...

class QuantizedConv2DTranspose(Layer):
    weight: Incomplete
    bias: Incomplete
    def __init__(self, layer, weight_bits: int = 8, activation_bits: int = 8, moving_rate: float = 0.9, weight_quantize_type: str = 'abs_max', activation_quantize_type: str = 'abs_max', weight_pre_layer: Incomplete | None = None, act_pre_layer: Incomplete | None = None, weight_quant_layer: Incomplete | None = None, act_quant_layer: Incomplete | None = None) -> None: ...
    def forward(self, input, output_size: Incomplete | None = None): ...

class QuantizedLinear(Layer):
    weight: Incomplete
    bias: Incomplete
    name: Incomplete
    def __init__(self, layer, weight_bits: int = 8, activation_bits: int = 8, moving_rate: float = 0.9, weight_quantize_type: str = 'abs_max', activation_quantize_type: str = 'abs_max', weight_pre_layer: Incomplete | None = None, act_pre_layer: Incomplete | None = None, weight_quant_layer: Incomplete | None = None, act_quant_layer: Incomplete | None = None) -> None: ...
    def forward(self, input): ...

class QuantizedColumnParallelLinear(Layer):
    weight: Incomplete
    bias: Incomplete
    name: Incomplete
    is_mp: Incomplete
    model_parallel_group: Incomplete
    gather_output: Incomplete
    def __init__(self, layer, weight_bits: int = 8, activation_bits: int = 8, moving_rate: float = 0.9, weight_quantize_type: str = 'abs_max', activation_quantize_type: str = 'abs_max', weight_pre_layer: Incomplete | None = None, act_pre_layer: Incomplete | None = None, weight_quant_layer: Incomplete | None = None, act_quant_layer: Incomplete | None = None) -> None: ...
    def forward(self, input): ...

class QuantizedRowParallelLinear(Layer):
    weight: Incomplete
    bias: Incomplete
    name: Incomplete
    input_is_parallel: Incomplete
    is_mp: Incomplete
    model_parallel_group: Incomplete
    def __init__(self, layer, weight_bits: int = 8, activation_bits: int = 8, moving_rate: float = 0.9, weight_quantize_type: str = 'abs_max', activation_quantize_type: str = 'abs_max', weight_pre_layer: Incomplete | None = None, act_pre_layer: Incomplete | None = None, weight_quant_layer: Incomplete | None = None, act_quant_layer: Incomplete | None = None) -> None: ...
    def forward(self, input): ...

class QuantizedMatmul(Layer):
    def __init__(self, layer: Incomplete | None = None, weight_bits: int = 8, activation_bits: int = 8, moving_rate: float = 0.9, weight_quantize_type: str = 'abs_max', activation_quantize_type: str = 'abs_max', weight_pre_layer: Incomplete | None = None, act_pre_layer: Incomplete | None = None, weight_quant_layer: Incomplete | None = None, act_quant_layer: Incomplete | None = None) -> None: ...
    def forward(self, x, y, transpose_x: bool = False, transpose_y: bool = False, name: Incomplete | None = None): ...

class MAOutputScaleLayer(Layer):
    def __init__(self, layer: Incomplete | None = None, moving_rate: float = 0.9, name: Incomplete | None = None, dtype: str = 'float32', reduce_type: Incomplete | None = None) -> None: ...
    def forward(self, *inputs, **kwargs): ...

class FakeQuantMAOutputScaleLayer(Layer):
    def __init__(self, layer, weight_bits: int = 8, activation_bits: int = 8, moving_rate: float = 0.9, name: Incomplete | None = None, reduce_type: Incomplete | None = None, *args, **kwargs) -> None: ...
    def forward(self, *inputs, **kwargs): ...
