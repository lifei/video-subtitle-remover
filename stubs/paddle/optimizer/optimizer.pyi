from ..base import framework as framework, unique_name as unique_name
from ..base.backward import append_backward as append_backward
from ..base.framework import Parameter as Parameter
from ..base.layer_helper import LayerHelper as LayerHelper
from .lr import LRScheduler as LRScheduler
from _typeshed import Incomplete
from paddle._pir_ops import parameter as parameter, set_parameter as set_parameter
from paddle.base import core as core
from paddle.base.framework import Variable as Variable, default_main_program as default_main_program, device_guard as device_guard, in_dygraph_mode as in_dygraph_mode, in_dynamic_or_pir_mode as in_dynamic_or_pir_mode, in_pir_mode as in_pir_mode, name_scope as name_scope, use_pir_api as use_pir_api
from paddle.regularizer import L2Decay as L2Decay

g_shard_bypass_dygraph_optimizer: Incomplete

def append_backward_new(loss_list, parameter_list: Incomplete | None = None, no_grad_set: Incomplete | None = None, callbacks: Incomplete | None = None, checkpoints: Incomplete | None = None, distop_context: Incomplete | None = None): ...

class Optimizer:
    regularization: Incomplete
    helper: Incomplete
    clear_gradients: Incomplete
    def __init__(self, learning_rate, parameters: Incomplete | None = None, weight_decay: Incomplete | None = None, grad_clip: Incomplete | None = None, name: Incomplete | None = None) -> None: ...
    def state_dict(self): ...
    def set_state_dict(self, state_dict) -> None: ...
    def get_opti_var_name_list(self): ...
    def set_lr(self, value) -> None: ...
    def set_lr_scheduler(self, scheduler) -> None: ...
    def get_lr(self): ...
    def backward(self, loss, startup_program: Incomplete | None = None, parameters: Incomplete | None = None, no_grad_set: Incomplete | None = None, callbacks: Incomplete | None = None): ...
    def apply_gradients(self, params_grads): ...
    def append_regularization_ops(self, parameters_and_grads, regularization: Incomplete | None = None): ...
    def clear_grad(self, set_to_zero: bool = True) -> None: ...
    def minimize(self, loss, startup_program: Incomplete | None = None, parameters: Incomplete | None = None, no_grad_set: Incomplete | None = None): ...
    def step(self): ...
