from . import core as core, framework as framework, log_helper as log_helper, unique_name as unique_name
from .data_feeder import check_type as check_type
from .framework import program_guard as program_guard
from .proto import framework_pb2 as framework_pb2
from _typeshed import Incomplete

class ProgramStats:
    block: Incomplete
    ops: Incomplete
    op_deps: Incomplete
    var_op_deps: Incomplete
    def __init__(self, block, ops) -> None: ...
    def get_input_nodes(self): ...
    def get_reserved_vars(self): ...
    def get_out_of_subgraph_vars(self, begin_op_idx, end_op_idx): ...
    def is_subgraph(self, var_group1, var_group2): ...
    def build_stats(self) -> None: ...
    def sort_checkpoints(self, checkpoints_name): ...
    def modify_forward_desc_for_recompute(self) -> None: ...

def serialize_op_decs(op_desc): ...
def infershape_for_composite(block, grad_op_desc) -> None: ...
def append_backward(loss, parameter_list: Incomplete | None = None, no_grad_set: Incomplete | None = None, callbacks: Incomplete | None = None, checkpoints: Incomplete | None = None, distop_context: Incomplete | None = None): ...
def calc_gradient_helper(targets, inputs, target_gradients: Incomplete | None = None, no_grad_set: Incomplete | None = None): ...
def calc_gradient(targets, inputs, target_gradients: Incomplete | None = None, no_grad_set: Incomplete | None = None): ...
def gradients(targets, inputs, target_gradients: Incomplete | None = None, no_grad_set: Incomplete | None = None): ...
def gradients_with_optimizer(program, optimizer, inputs: Incomplete | None = None, outputs: Incomplete | None = None): ...
