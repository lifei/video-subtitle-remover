from .amp_nn import check_finite_and_unscale as check_finite_and_unscale, update_loss_scaling as update_loss_scaling
from .fp16_lists import AutoMixedPrecisionLists as AutoMixedPrecisionLists, check_amp_dtype as check_amp_dtype
from .fp16_utils import cast_model_to_fp16 as cast_model_to_fp16, cast_parameters_to_fp16 as cast_parameters_to_fp16, update_role_var_grad as update_role_var_grad
from .function_overload import FunctionType as FunctionType, overload as overload
from _typeshed import Incomplete
from paddle.base import core as core, default_main_program as default_main_program, default_startup_program as default_startup_program, program_guard as program_guard, unique_name as unique_name

class OptimizerWithMixedPrecision:
    use_promote: Incomplete
    def __init__(self, optimizer, amp_lists, level, dtype, init_loss_scaling, use_dynamic_loss_scaling, incr_every_n_steps, decr_every_n_nan_or_inf, incr_ratio, decr_ratio, use_amp_guard: Incomplete | None = None, use_master_grad: bool = False, use_promote: bool = False) -> None: ...
    def get_loss_scaling(self): ...
    def get_scaled_loss(self): ...
    def backward(self, loss, startup_program: Incomplete | None = None, parameter_list: Incomplete | None = None, no_grad_set: Incomplete | None = None, callbacks: Incomplete | None = None): ...
    def amp_init(self, place, scope: Incomplete | None = None, test_program: Incomplete | None = None, use_fp16_test: bool = False, rewrite_master_weight: bool = False) -> None: ...
    def apply_gradients(self, params_grads): ...
    def apply_optimize(self, loss, startup_program, params_grads): ...
    def minimize(self, loss, startup_program: Incomplete | None = None, parameter_list: Incomplete | None = None, no_grad_set: Incomplete | None = None): ...
