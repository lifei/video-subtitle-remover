from .amp_lists import black_list as black_list, white_list as white_list
from _typeshed import Incomplete
from collections.abc import Generator
from paddle.base import core as core
from paddle.base.framework import dygraph_only as dygraph_only
from paddle.base.wrapped_decorator import signature_safe_contextmanager as signature_safe_contextmanager

AMP_RELATED_FLAGS: Incomplete
AMP_RELATED_FLAGS_SETTING: Incomplete
AMP_LEVEL: Incomplete

def amp_state(): ...

class AMPGlobalState:
    model_parameters: Incomplete
    use_master_grad: bool
    already_register_final_backward_hook: bool
    amp_dtype: str
    def __init__(self) -> None: ...
    def __setattr__(self, name, val) -> None: ...

def amp_global_state(): ...
def need_keep_fp32(layer, dtype): ...
def set_excluded_layers(models, excluded_layers) -> None: ...
def amp_initialize(models, dtype, excluded_layers): ...
def check_models(models) -> None: ...
def check_optimizers(optimizers) -> None: ...
def amp_guard(enable: bool = True, custom_white_list: Incomplete | None = None, custom_black_list: Incomplete | None = None, level: str = 'O1', dtype: str = 'float16', use_promote: bool = True) -> Generator[None]: ...

class StateDictHook:
    def __init__(self, save_dtype) -> None: ...
    def __call__(self, state_dict) -> None: ...

def amp_decorate(models, optimizers: Incomplete | None = None, level: str = 'O1', dtype: str = 'float16', master_weight: Incomplete | None = None, save_dtype: Incomplete | None = None, master_grad: bool = False, excluded_layers: Incomplete | None = None): ...
def auto_cast(enable: bool = True, custom_white_list: Incomplete | None = None, custom_black_list: Incomplete | None = None, level: str = 'O1', dtype: str = 'float16', use_promote: bool = True): ...
def decorate(models, optimizers: Incomplete | None = None, level: str = 'O1', dtype: str = 'float16', master_weight: Incomplete | None = None, save_dtype: Incomplete | None = None, master_grad: bool = False, excluded_layers: Incomplete | None = None): ...
