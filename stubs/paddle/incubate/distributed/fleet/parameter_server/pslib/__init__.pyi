from .optimizer_factory import DistributedAdam as DistributedAdam, FLEET_GLOBAL_DICT as FLEET_GLOBAL_DICT
from _typeshed import Incomplete
from paddle.common_ops_import import LayerHelper as LayerHelper
from paddle.framework import core as core
from paddle.incubate.distributed.fleet.base import DistributedOptimizer as DistributedOptimizer, Fleet as Fleet, Mode as Mode
from paddle.incubate.distributed.fleet.role_maker import HeterRoleMaker as HeterRoleMaker, MPISymetricRoleMaker as MPISymetricRoleMaker

class PSLib(Fleet):
    def __init__(self) -> None: ...
    def init(self, role_maker: Incomplete | None = None) -> None: ...
    def set_pull_local_thread_num(self, thread_num) -> None: ...
    all_ips_: Incomplete
    def init_worker(self) -> None: ...
    def init_server(self, model_dir: Incomplete | None = None, **kwargs) -> None: ...
    def run_server(self) -> None: ...
    def end_pass(self, scope) -> None: ...
    def train_from_dataset(self, executor, program: Incomplete | None = None, dataset: Incomplete | None = None, scope: Incomplete | None = None, thread: int = 0, debug: bool = False, fetch_list: Incomplete | None = None, fetch_info: Incomplete | None = None, print_period: int = 100, fetch_handler: Incomplete | None = None) -> None: ...
    def start_heter_trainer(self, executor, program: Incomplete | None = None, scope: Incomplete | None = None, debug: bool = False, fetch_list: Incomplete | None = None, fetch_info: Incomplete | None = None, print_period: int = 100, fetch_handler: Incomplete | None = None) -> None: ...
    def stop_worker(self) -> None: ...
    def distributed_optimizer(self, optimizer, strategy={}): ...
    def save_inference_model(self, executor, dirname, feeded_var_names: Incomplete | None = None, target_vars: Incomplete | None = None, main_program: Incomplete | None = None, export_for_deployment: bool = True) -> None: ...
    def print_table_stat(self, table_id, pass_id, threshold) -> None: ...
    def set_file_num_one_shard(self, table_id, file_num) -> None: ...
    def save_persistables(self, executor, dirname, main_program: Incomplete | None = None, **kwargs) -> None: ...
    def save_model_with_whitelist(self, executor, dirname, whitelist_path, main_program: Incomplete | None = None, **kwargs) -> None: ...
    def save_multi_table_one_path(self, table_ids, model_dir, **kwargs) -> None: ...
    def save_cache_model(self, executor, dirname, main_program: Incomplete | None = None, **kwargs): ...
    def shrink_sparse_table(self) -> None: ...
    def shrink_dense_table(self, decay, emb_dim: int = 11, scope: Incomplete | None = None, table_id: Incomplete | None = None) -> None: ...
    def clear_one_table(self, table_id) -> None: ...
    def clear_model(self) -> None: ...
    def load_pslib_whitelist(self, table_id, model_path, **kwargs) -> None: ...
    def load_one_table(self, table_id, model_path, **kwargs) -> None: ...
    def confirm(self) -> None: ...
    def revert(self) -> None: ...
    def load_model(self, model_dir: Incomplete | None = None, **kwargs) -> None: ...
    def save_model(self, model_dir: Incomplete | None = None, **kwargs) -> None: ...
    def save_one_table(self, table_id, model_dir, **kwargs) -> None: ...
    def set_date(self, table_id, date) -> None: ...

fleet: Incomplete

class fleet_embedding:
    origin_emb_v2: Incomplete
    click_name: Incomplete
    scale_sparse_grad: Incomplete
    accessor: str
    def __init__(self, click_name, scale_sparse_grad: bool = True) -> None: ...
    def __enter__(self) -> None: ...
    def __exit__(self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: types.TracebackType | None) -> None: ...

class DownpourOptimizer(DistributedOptimizer):
    def __init__(self, optimizer, strategy: Incomplete | None = None) -> None: ...
    def backward(self, loss, startup_program: Incomplete | None = None, parameter_list: Incomplete | None = None, no_grad_set: Incomplete | None = None, callbacks: Incomplete | None = None) -> None: ...
    def apply_gradients(self, params_grads) -> None: ...
    def get_dist_env(self): ...
    def minimize(self, losses, scopes: Incomplete | None = None, startup_programs: Incomplete | None = None, parameter_list: Incomplete | None = None, no_grad_set: Incomplete | None = None, program_mode: str = 'all_reduce'): ...
