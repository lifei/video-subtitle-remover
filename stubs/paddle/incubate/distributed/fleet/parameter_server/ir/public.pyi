from _typeshed import Incomplete
from paddle.framework import core as core
from paddle.incubate.distributed.fleet.parameter_server.ir import vars_metatools as vars_metatools
from paddle.incubate.distributed.fleet.parameter_server.ir.ps_dispatcher import RoundRobin as RoundRobin
from paddle.incubate.distributed.fleet.parameter_server.mode import DistributedMode as DistributedMode

OP_NAME_SCOPE: str
CLIP_OP_NAME_SCOPE: str
STEP_COUNTER: str
LEARNING_RATE_DECAY_COUNTER: str
OP_ROLE_VAR_ATTR_NAME: Incomplete
RPC_OP_ROLE_ATTR_NAME: Incomplete
RPC_OP_ROLE_ATTR_VALUE: Incomplete
op_role_attr_name: Incomplete
LR_SCHED_OP_ROLE_ATTR_VALUE: Incomplete
OPT_OP_ROLE_ATTR_VALUE: Incomplete
SPARSE_OP_LIST: Incomplete
SPARSE_OP_TYPE_DICT: Incomplete

def is_sparse_op(op): ...
def is_distributed_sparse_op(op): ...
def get_sparse_tablename(op): ...
def get_sparse_tablenames(program, is_distributed): ...

class MergedVariable:
    merged_var: Incomplete
    ordered_vars: Incomplete
    offsets: Incomplete
    def __init__(self, merged, ordered, offsets) -> None: ...

def Singleton(cls): ...

class CompileTimeStrategy:
    min_block_size: int
    origin_main_program: Incomplete
    origin_startup_program: Incomplete
    origin_ps_main_program: Incomplete
    origin_ps_startup_program: Incomplete
    strategy: Incomplete
    role_maker: Incomplete
    use_ps_gpu: bool
    is_heter_ps_mode: Incomplete
    origin_sparse_pairs: Incomplete
    origin_dense_pairs: Incomplete
    merged_variables_pairs: Incomplete
    merged_dense_pairs: Incomplete
    merged_sparse_pairs: Incomplete
    merged_variable_map: Incomplete
    param_name_to_grad_name: Incomplete
    grad_name_to_param_name: Incomplete
    param_grad_ep_mapping: Incomplete
    grad_param_mapping: Incomplete
    tensor_table_dict: Incomplete
    origin_merged_variables_pairs: Incomplete
    origin_merged_dense_pairs: Incomplete
    origin_merged_sparse_pairs: Incomplete
    def __init__(self, main_program, startup_program, strategy, role_maker) -> None: ...
    def get_distributed_mode(self): ...
    def is_sync_mode(self): ...
    def is_geo_mode(self): ...
    def is_async_mode(self): ...
    def get_role_id(self): ...
    def get_trainers(self): ...
    def get_ps_endpoint(self): ...
    def get_ps_endpoints(self): ...
    def get_heter_worker_endpoints(self): ...
    def get_next_stage_trainers(self): ...
    def get_heter_worker_endpoint(self): ...
    def get_trainer_endpoints(self): ...
    def get_trainer_endpoint(self): ...
    def get_previous_stage_trainers(self): ...
    def get_origin_programs(self): ...
    def get_origin_main_program(self): ...
    def get_origin_startup_program(self): ...
    def set_origin_ps_main_program(self, program) -> None: ...
    def set_origin_ps_startup_program(self, program) -> None: ...
    def get_origin_ps_main_program(self): ...
    def get_origin_ps_startup_program(self): ...
    def add_tensor_table(self, feed_var_name, fetch_var_name: str = '', startup_program: Incomplete | None = None, main_program: Incomplete | None = None, tensor_table_class: str = '') -> None: ...
    def get_tensor_table_dict(self): ...
    def get_sparse_varname_on_ps(self, is_distributed, endpoint: Incomplete | None = None): ...
    def get_optimize_varname_on_ps(self, param_name): ...
    def build_ctx(self, vars, mapping, is_grad, is_sparse, is_send, is_distributed: bool = False): ...
    def get_trainer_send_context(self): ...
    def get_communicator_send_context(self): ...
    def get_communicator_recv_context(self, recv_type: int = 1, use_origin_program: bool = False): ...
    def get_the_one_trainer_send_context(self, split_dense_table): ...
    def get_dense_send_context(self, send_ctx, idx, merged_dense_pairs, trainer_id, split_dense_table: bool = False): ...
    def get_the_one_send_context(self, split_dense_table: bool = False, use_origin_program: bool = False, ep_list: Incomplete | None = None): ...
    def get_the_one_recv_context(self, is_dense: bool = True, split_dense_table: bool = False, use_origin_program: bool = False): ...
    def get_server_runtime_config(self): ...
    def get_var_distributed(self, varname, is_param): ...
    def get_param_grads(self): ...
    def remove_var_pair_by_grad(self, var_name) -> None: ...
