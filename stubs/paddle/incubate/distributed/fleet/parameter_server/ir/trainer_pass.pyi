from _typeshed import Incomplete
from paddle import framework as framework
from paddle.distributed.transpiler.details.program_utils import delete_ops as delete_ops
from paddle.framework import core as core
from paddle.incubate.distributed.fleet.parameter_server.ir.public import get_sparse_tablenames as get_sparse_tablenames
from paddle.incubate.distributed.fleet.parameter_server.mode import DistributedMode as DistributedMode

OP_NAME_SCOPE: str
CLIP_OP_NAME_SCOPE: str
STEP_COUNTER: str
OP_ROLE_VAR_ATTR_NAME: Incomplete
RPC_OP_ROLE_ATTR_NAME: Incomplete
RPC_OP_ROLE_ATTR_VALUE: Incomplete
LR_SCHED_OP_ROLE_ATTR_VALUE: Incomplete
OPT_OP_ROLE_ATTR_VALUE: Incomplete
op_role_attr_name: Incomplete
SPARSE_OP_TYPE_DICT: Incomplete
SPARSE_GRAD_OP_TYPE_DICT: Incomplete
DEVICE_LIST: Incomplete
COMMUNICATE_OPS_TYPE: Incomplete
DEFAULT_DEVICE: str

def delete_optimizer_pass(program, config): ...
def distributed_ops_pass(program, config, use_ps_gpu: bool = False): ...
def append_send_ops_pass(program, config): ...
def init_from_server_pass(program, config): ...
def fake_init_ops_pass(program, config): ...
def ps_gpu_pass(program): ...
def delete_extra_optimizes_pass(program, config): ...
def find_heter_ops(program, default_device: str = 'cpu'): ...
def create_heter_program(program, config, heter_program, program_block_ops_list, heter_ops, block_var_detail, current_device, stage_id) -> None: ...
def check_heter_compile_time_strategy(program, config, send_grad_var_list) -> None: ...
def create_trainer_program(program, origin_program, config, program_block_ops_list, block_var_detail) -> None: ...
def insert_communicate_op(orign_program, config, heter_block, stage_id, first_op_index, block_var_detail, device, is_forward: bool = True): ...
def create_backward_block(program, origin_program, config, bp_ops_list, block_var_detail): ...
def replace_ops_by_communicate_op(program, config, heter_block_index, ops_list, block_var_detail): ...
def remove_trainer_send_op(program, config, heter_block_index, block_var_detail) -> None: ...
def add_heter_send_op(program, heter_program, block, block_var_detail): ...
def find_send_op(program): ...
def get_communicate_var_info(program, block_index, entrance_var_list, type: str = 'forward'): ...
def union_forward_gradient_op(program_block_ops_list): ...
def find_block_joints(program, program_block_ops_list, heter_ops): ...
def find_entrance_exit_private(program, program_block_ops_list): ...
def entrance_exit_check(program, program_block_ops_list, block_var_detail, heter_ops): ...
def find_need_var_from_previous_block(need_add_vars, block_var_detail, current_index, heter_ops): ...
def delete_block_useless_exit(program, program_block_ops_list, block_var_detail): ...
def check_op_device(block, device) -> None: ...
def screen_persistables(program, var_list): ...
def insert_reshape_op(program, block, index, var_name, new_var_name, new_var_shape: Incomplete | None = None) -> None: ...
def insert_send_concat_op(program, block, index, var_name_list, new_var_name, new_var_shape) -> None: ...
def insert_recv_slice_op(program, block, index, var_name, var_shape, dtype, type, new_var_name_list, new_var_shape_list) -> None: ...
def add_heter_trainer_useful_vars(config, program, heter_program, heter_block, static_var) -> None: ...
def delete_trainer_useless_var(config, program, static_var): ...
def block_append_op(program, origin_program, block, op): ...
def add_vars_by_var_list(var_name_list, origin_program, program, block) -> None: ...
def get_varlist_from_op_map(var_map): ...
def find_ops_list_input_output(program, ops_list): ...
def find_op_input_output(program, block, op): ...
def get_vars_name_in_block(block): ...
def is_same_op(op1, op2): ...
def delete_same_ops(block, ops) -> None: ...
