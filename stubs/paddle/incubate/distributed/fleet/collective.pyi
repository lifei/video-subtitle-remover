import abc
from _typeshed import Incomplete
from paddle import base as base
from paddle.base.compiler import CompiledProgram as CompiledProgram
from paddle.base.executor import Executor as Executor
from paddle.base.framework import Program as Program
from paddle.base.incubate.checkpoint.checkpoint_saver import CheckpointSaver as CheckpointSaver, PaddleModel as PaddleModel
from paddle.distributed.fleet.meta_optimizers import RawProgramOptimizer as RawProgramOptimizer
from paddle.incubate.distributed.fleet.base import DistributedOptimizer as DistributedOptimizer, Fleet as Fleet, Mode as Mode
from paddle.static import io as io

class Collective(Fleet):
    startup_program: Incomplete
    main_program: Incomplete
    def __init__(self) -> None: ...
    def init_worker(self) -> None: ...
    def run_worker(self, main_programs: Incomplete | None = None, scopes: Incomplete | None = None) -> None: ...
    def init_server(self, model_dir: Incomplete | None = None) -> None: ...
    def run_server(self) -> None: ...
    def stop_worker(self) -> None: ...
    def distributed_optimizer(self, optimizer, strategy: Incomplete | None = None): ...
    def save_inference_model(self, executor, path_prefix, feeded_vas: Incomplete | None = None, fetch_vars: Incomplete | None = None, program: Incomplete | None = None, legacy_format: bool = False) -> None: ...
    def save_persistables(self, executor, dirname, main_program: Incomplete | None = None, filename: Incomplete | None = None) -> None: ...
    def save_checkpoint(self, executor, path, trainer_id, train_status, fs, main_program: Incomplete | None = None, local_cache_path: str = '.cache', remain_all_checkpoint: bool = True): ...
    def load_checkpoint(self, executor, path, trainer_id, train_status, fs, main_program: Incomplete | None = None, local_cache_path: str = '.cache', ignore_empty: bool = True): ...

fleet: Incomplete

class DistributedStrategy(base.BuildStrategy):
    use_local_sgd: bool
    use_dist_fc: bool
    dist_fc_config: Incomplete
    mode: str
    collective_mode: Incomplete
    nccl_comm_num: int
    forward_recompute: bool
    recompute_checkpoints: Incomplete
    use_amp: bool
    amp_loss_scaling: Incomplete
    exec_strategy: Incomplete
    def __init__(self) -> None: ...

class CollectiveOpBasedOptimizer(DistributedOptimizer, metaclass=abc.ABCMeta):
    def __init__(self, optimizer, strategy: Incomplete | None = None) -> None: ...
    def backward(self, loss, startup_program: Incomplete | None = None, parameter_list: Incomplete | None = None, no_grad_set: Incomplete | None = None, callbacks: Incomplete | None = None): ...
    def apply_gradients(self, params_grads): ...

class CollectiveOptimizer(DistributedOptimizer):
    print_config: bool
    def __init__(self, optimizer, strategy=...) -> None: ...
    def backward(self, loss, startup_program: Incomplete | None = None, parameter_list: Incomplete | None = None, no_grad_set: Incomplete | None = None, callbacks: Incomplete | None = None): ...
    def apply_gradients(self, params_grads): ...
    def raiseOptimizeError(self, strategy_name, optimize_name) -> None: ...
    def minimize(self, loss, startup_program: Incomplete | None = None, parameter_list: Incomplete | None = None, no_grad_set: Incomplete | None = None): ...
