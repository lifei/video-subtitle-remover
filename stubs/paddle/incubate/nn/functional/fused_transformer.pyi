from _typeshed import Incomplete
from paddle.base import core as core
from paddle.base.data_feeder import check_dtype as check_dtype, check_variable_and_dtype as check_variable_and_dtype
from paddle.base.framework import default_main_program as default_main_program
from paddle.base.layer_helper import LayerHelper as LayerHelper
from paddle.framework import in_dynamic_mode as in_dynamic_mode, in_dynamic_or_pir_mode as in_dynamic_or_pir_mode

def fused_feedforward(x, linear1_weight, linear2_weight, linear1_bias: Incomplete | None = None, linear2_bias: Incomplete | None = None, ln1_scale: Incomplete | None = None, ln1_bias: Incomplete | None = None, ln2_scale: Incomplete | None = None, ln2_bias: Incomplete | None = None, dropout1_rate: float = 0.5, dropout2_rate: float = 0.5, activation: str = 'relu', ln1_epsilon: float = 1e-05, ln2_epsilon: float = 1e-05, pre_layer_norm: bool = False, training: bool = True, mode: str = 'upscale_in_train', ring_id: int = -1, add_residual: bool = True, name: Incomplete | None = None): ...
def fused_bias_dropout_residual_layer_norm(x, residual, bias: Incomplete | None = None, ln_scale: Incomplete | None = None, ln_bias: Incomplete | None = None, dropout_rate: float = 0.5, ln_epsilon: float = 1e-05, training: bool = True, mode: str = 'upscale_in_train', name: Incomplete | None = None): ...
def fused_multi_head_attention(x, qkv_weight, linear_weight, pre_layer_norm: bool = False, pre_ln_scale: Incomplete | None = None, pre_ln_bias: Incomplete | None = None, ln_scale: Incomplete | None = None, ln_bias: Incomplete | None = None, pre_ln_epsilon: float = 1e-05, qkv_bias: Incomplete | None = None, linear_bias: Incomplete | None = None, cache_kv: Incomplete | None = None, attn_mask: Incomplete | None = None, dropout_rate: float = 0.5, attn_dropout_rate: float = 0.5, ln_epsilon: float = 1e-05, training: bool = True, mode: str = 'upscale_in_train', ring_id: int = -1, add_residual: bool = True, num_heads: int = -1, transpose_qkv_wb: bool = False, name: Incomplete | None = None): ...
def fused_multi_transformer(x, ln_scales, ln_biases, qkv_weights, qkv_biases, linear_weights, linear_biases, ffn_ln_scales, ffn_ln_biases, ffn1_weights, ffn1_biases, ffn2_weights, ffn2_biases, pre_layer_norm: bool = True, epsilon: float = 1e-05, cache_kvs: Incomplete | None = None, pre_caches: Incomplete | None = None, seq_lens: Incomplete | None = None, rotary_embs: Incomplete | None = None, time_step: Incomplete | None = None, attn_mask: Incomplete | None = None, dropout_rate: float = 0.0, rotary_emb_dims: int = 0, activation: str = 'gelu', training: bool = False, mode: str = 'upscale_in_train', trans_qkvw: bool = True, ring_id: int = -1, name: Incomplete | None = None): ...
