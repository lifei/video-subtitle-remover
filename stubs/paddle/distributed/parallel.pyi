from . import parallel_helper as parallel_helper
from .backup_env import getenv_or_backup as getenv_or_backup
from _typeshed import Incomplete
from collections.abc import Generator
from paddle import framework as framework
from paddle.distributed.collective import Group as Group
from paddle.distributed.communication.group import is_initialized as is_initialized
from paddle.distributed.fleet.base.private_helper_function import wait_server_ready as wait_server_ready
from paddle.distributed.fleet.launch_utils import check_backend as check_backend
from paddle.framework import core as core, in_dynamic_mode as in_dynamic_mode
from paddle.nn.layer import layers as layers
from paddle.utils import deprecated as deprecated

ParallelStrategy: Incomplete

def build_groups(vars, group_size): ...
def sync_params_buffers(model, comm_group: Incomplete | None = None, src_rank: int = 0, is_model_parallel: bool = False, fuse_params: bool = True) -> None: ...

class DataParallel(layers.Layer):
    find_unused_parameters: Incomplete
    grad_need_sync: bool
    group: Incomplete
    var_dtype: Incomplete
    comm_buffer_size: Incomplete
    last_comm_buffer_size: Incomplete
    def __init__(self, layers, strategy: Incomplete | None = None, comm_buffer_size: int = 25, last_comm_buffer_size: int = 1, find_unused_parameters: bool = False, group: Incomplete | None = None) -> None: ...
    group_indices: Incomplete
    def init_reducer(self): ...
    def no_sync(self) -> Generator[None]: ...
    def forward(self, *inputs, **kwargs): ...
    def scale_loss(self, loss): ...
    def apply_collective_grads(self) -> None: ...
    def state_dict(self, destination: Incomplete | None = None, include_sublayers: bool = True, structured_name_prefix: str = ''): ...
    def set_state_dict(self, state_dict, use_structured_name: bool = True) -> None: ...
    set_dict = set_state_dict
    load_dict = set_state_dict

class ParallelEnv:
    def __init__(self) -> None: ...
    @property
    def rank(self): ...
    @property
    def world_size(self): ...
    @property
    def device_id(self): ...
    @property
    def device_type(self): ...
    @property
    def current_endpoint(self): ...
    @property
    def trainer_endpoints(self): ...
    @property
    def nrings(self): ...
    @property
    def pg_timeout(self): ...
    local_rank = rank
    nranks = world_size
    dev_id = device_id

def init_parallel_env(): ...
def get_rank(group: Incomplete | None = None): ...
def get_world_size(group: Incomplete | None = None): ...
