from .base.distributed_strategy import DistributedStrategy as DistributedStrategy
from .base.meta_optimizer_factory import MetaOptimizerFactory as MetaOptimizerFactory
from .base.role_maker import PaddleCloudRoleMaker as PaddleCloudRoleMaker, RoleMakerBase as RoleMakerBase
from .base.runtime_factory import RuntimeFactory as RuntimeFactory
from .base.strategy_compiler import StrategyCompiler as StrategyCompiler
from .meta_parallel import model_parallel_random_seed as model_parallel_random_seed
from .utils.log_util import logger as logger, set_log_level as set_log_level
from _typeshed import Incomplete
from paddle.base import compiler as compiler
from paddle.base.wrapped_decorator import wrap_decorator as wrap_decorator
from paddle.framework import in_dynamic_mode as in_dynamic_mode
from paddle.framework.ir import apply_build_strategy as apply_build_strategy

def apply_ir_passes(main_program, startup_program, config): ...

inited_runtime_handler: Incomplete
is_non_distributed_check: Incomplete

class Fleet:
    strategy_compiler: Incomplete
    user_defined_optimizer: Incomplete
    def __init__(self) -> None: ...
    def init(self, role_maker: Incomplete | None = None, is_collective: bool = False, strategy: Incomplete | None = None, log_level: str = 'INFO'): ...
    def allreduce_perf(self, iteration, x, group, perf_size, perf_threshold_time, warmup: bool = False) -> None: ...
    def reduce_perf(self, iteration, x, group, perf_size, perf_threshold_time) -> None: ...
    def broadcast_perf(self, iteration, x, group, perf_size, perf_threshold_time) -> None: ...
    def allgather_perf(self, iteration, x, group, perf_size, perf_threshold_time) -> None: ...
    def reduce_scatter_perf(self, iteration, x, group, perf_size, perf_threshold_time) -> None: ...
    def collective_perf(self, comm_type, round: int = 50, size_and_time={}) -> None: ...
    def get_hybrid_communicate_group(self): ...
    def get_hybrid_parallel_topology(self): ...
    def is_first_worker(self): ...
    def worker_index(self): ...
    def worker_num(self): ...
    def node_num(self): ...
    def local_rank(self): ...
    def local_device_ids(self): ...
    def world_device_ids(self): ...
    def is_worker(self): ...
    def is_coordinator(self): ...
    def worker_endpoints(self, to_string: bool = False): ...
    def server_num(self): ...
    def server_index(self): ...
    def server_endpoints(self, to_string: bool = False): ...
    def is_server(self): ...
    def barrier_worker(self) -> None: ...
    def all_reduce(self, input, mode: str = 'sum'): ...
    def init_worker(self, scopes: Incomplete | None = None) -> None: ...
    def init_coordinator(self, scopes: Incomplete | None = None) -> None: ...
    def make_fl_strategy(self) -> None: ...
    def get_fl_client(self): ...
    def init_server(self, *args, **kwargs) -> None: ...
    def load_model(self, path, mode) -> None: ...
    def load_one_table(self, table_id, path, mode) -> None: ...
    def load_inference_model(self, path, mode) -> None: ...
    def run_server(self) -> None: ...
    def stop_worker(self) -> None: ...
    def save(self, dirname, feed=[], fetch=[], **configs) -> None: ...
    def save_inference_model(self, executor, dirname, feeded_var_names, target_vars, main_program: Incomplete | None = None, export_for_deployment: bool = True, mode: int = 0) -> None: ...
    def save_persistables(self, executor, dirname, main_program: Incomplete | None = None, mode: int = 0) -> None: ...
    def save_cache_model(self, dirname, **configs): ...
    def check_save_pre_patch_done(self): ...
    def save_cache_table(self, table_id, pass_id, mem_cache_key_threshold: int = 4000000000): ...
    def save_one_table(self, table_id, path, mode) -> None: ...
    def save_dense_params(self, executor, dirname, scope, program, var_names: Incomplete | None = None) -> None: ...
    def shrink(self, threshold: Incomplete | None = None) -> None: ...
    def distributed_optimizer(self, optimizer, strategy: Incomplete | None = None): ...
    def get_loss_scaling(self): ...
    def amp_init(self, place, scope: Incomplete | None = None, test_program: Incomplete | None = None, use_fp16_test: bool = False): ...
    def qat_init(self, place, scope: Incomplete | None = None, test_program: Incomplete | None = None): ...
    def minimize(self, loss, startup_program: Incomplete | None = None, parameter_list: Incomplete | None = None, no_grad_set: Incomplete | None = None): ...
