from ..meta_optimizers.dygraph_optimizer import HybridParallelOptimizer as HybridParallelOptimizer
from ..utils.hybrid_parallel_util import broadcast_dp_parameters as broadcast_dp_parameters, broadcast_mp_parameters as broadcast_mp_parameters, broadcast_sep_parameters as broadcast_sep_parameters, broadcast_sharding_parameters as broadcast_sharding_parameters
from ..utils.log_util import logger as logger
from .meta_parallel_base import MetaParallelBase as MetaParallelBase
from .parallel_layers.pp_layers import PipelineLayer as PipelineLayer
from _typeshed import Incomplete
from paddle import framework as framework
from paddle.distributed.fleet.utils.tensor_fusion_helper import FusedCommBuffer as FusedCommBuffer, HOOK_ACTION as HOOK_ACTION, assign_group_by_size as assign_group_by_size

g_shard_use_reduce: Incomplete

def get_action(is_dp, shard_split_param: bool = False): ...

class FakeMicroDataset:
    def __init__(self, data, is_first_stage, is_last_stage, acc_steps, micro_batch_size) -> None: ...
    def __iter__(self): ...
    def __next__(self): ...

class PipelineParallel(MetaParallelBase):
    use_data_parallel: Incomplete
    use_model_parallel: Incomplete
    use_sep_parallel: Incomplete
    use_sharding_parallel: Incomplete
    total_loss: Incomplete
    micro_batch_size: Incomplete
    accumulate_steps: Incomplete
    num_stages: Incomplete
    stage_id: Incomplete
    global_rank: Incomplete
    pp_group: Incomplete
    dp_group: Incomplete
    sharding_group: Incomplete
    timers: Incomplete
    micro_batch_id: int
    def __init__(self, layers, hcg, strategy) -> None: ...
    def is_pipeline_first_stage(self, ignore_virtual: bool = False): ...
    def is_pipeline_last_stage(self, ignore_virtual: bool = False): ...
    def set_virtual_pipeline_rank(self, rank) -> None: ...
    def fused_gradient(self, model, comm_group, acc_steps, dp, group_size=...): ...
    def bw_hook_func(self, buffer, param): ...
    def register_allreduce_overlap_hook(self, model, comm_group, acc_steps, dp, group_size=...) -> None: ...
    def timer_printer(self) -> None: ...
    scaler: Incomplete
    def forward_backward_pipeline(self, data, scaler: Incomplete | None = None, static_scheduler: bool = False): ...
    optimizer: Incomplete
    def register_sharding_comm_overlap_hook(self, optimizer) -> None: ...
    def train_batch(self, data, optimizer, lr_scheduler: Incomplete | None = None, scaler: Incomplete | None = None): ...
    train_loss: Incomplete
    def eval_batch(self, data, compute_loss: bool = False): ...
    def get_static_scheduler(self): ...

class PipelineParallelWithInterleave(PipelineParallel):
    num_model_chunks: Incomplete
    model_chunks: Incomplete
    def __init__(self, layers, hcg, strategy) -> None: ...
    def bw_hook_func(self, buffer, param): ...
    def register_allreduce_overlap_hook(self, model, comm_group, acc_steps, dp) -> None: ...
    scaler: Incomplete
    total_loss: Incomplete
    micro_batch_id: int
    input_tensors: Incomplete
    output_tensors: Incomplete
    output_tensor_grads: Incomplete
    def forward_backward_pipeline(self, data, scaler, forward_only: bool = False, compute_loss: bool = True, static_scheduler: bool = False): ...
    def train_batch(self, data, optimizer, lr_scheduler: Incomplete | None = None, scaler: Incomplete | None = None): ...
    def eval_batch(self, data, compute_loss: bool = False): ...
    def get_static_scheduler(self): ...

class PipelineParallelWithInterleaveFthenB(PipelineParallelWithInterleave):
    def __init__(self, layers, hcg, strategy) -> None: ...
    scaler: Incomplete
    total_loss: Incomplete
    micro_batch_id: int
    input_tensors: Incomplete
    output_tensors: Incomplete
    output_tensor_grads: Incomplete
    def forward_backward_pipeline(self, data, scaler, forward_only: bool = False, compute_loss: bool = True): ...
