from ...utils.log_util import logger as logger
from .utils import number_2_dtype as number_2_dtype, paddle_2_number as paddle_2_number
from _typeshed import Incomplete
from paddle import framework as framework

def initialize_p2p_groups(hcg, enable_partial_send_recv: bool = True, enable_timer: bool = False) -> None: ...

class SendRecvMeta:
    send_shape_message: Incomplete
    send_dtype_message: Incomplete
    recv_shape_message: Incomplete
    recv_dtype_message: Incomplete
    recv_stop_gradient: Incomplete
    has_send_meta: bool
    has_recv_meta: bool
    def __init__(self) -> None: ...
    def recv_meta(self, group) -> None: ...
    def send_meta(self, tensor, group) -> None: ...
    def set_send_message(self, tensor) -> None: ...

def send_partial(tensor, dst: int = 0, nranks: int = 1, rank_id: int = 0, group: Incomplete | None = None, use_calc_stream: bool = True): ...
def recv_partial(tensor, src: int = 0, nranks: int = 1, rank_id: int = 0, group: Incomplete | None = None, use_calc_stream: bool = True): ...
def allgather_partial(tensor, nranks: int = 1, rank_id: int = 0, group: Incomplete | None = None, use_calc_stream: bool = True): ...

class P2pHelper:
    def __init__(self, use_cache: bool = True) -> None: ...
    def recv_forward(self, pp_first_stage, sync_recv: bool = True): ...
    def recv_backward(self, pp_last_stage, sync_recv: bool = True): ...
    def send_forward(self, output_tensor, pp_last_stage) -> None: ...
    def send_backward(self, input_tensor_grad, pp_first_stage) -> None: ...
    def send_forward_recv_backward(self, output_tensor, pp_last_stage): ...
    def send_backward_recv_forward(self, input_tensor_grad, pp_first_stage): ...
    def send_forward_backward_recv_forward_backward(self, output_tensor, input_tensor_grad, recv_prev, recv_next): ...
    def send_forward_recv_forward(self, output_tensor, recv_prev): ...
    def send_backward_recv_backward(self, input_tensor_grad, recv_next): ...
