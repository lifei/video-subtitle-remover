from ...utils.log_util import logger as logger
from ...utils.tensor_fusion_helper import FusedCommBuffer as FusedCommBuffer, HOOK_ACTION as HOOK_ACTION, assign_group_by_size as assign_group_by_size, fused_parameters as fused_parameters
from _typeshed import Incomplete
from paddle import framework as framework
from paddle.base.framework import EagerParamBase as EagerParamBase
from paddle.distributed import fleet as fleet

g_shard_use_reduce: Incomplete
g_shard_norm_align_dp: Incomplete

class DygraphShardingOptimizer:
    tensor_fusion: Incomplete
    accumulate_steps: Incomplete
    comm_overlap: Incomplete
    fuse_optimizer: Incomplete
    origin_decay_param_fun: Incomplete
    def __init__(self, optimizer, hcg) -> None: ...
    def clear_grad(self, set_to_zero: bool = True) -> None: ...
    def filter_parameters(self, parameter_list, hcg): ...
    def reduce_gradients(self, parameter_list, hcg) -> None: ...
    def minimize(self, loss, startup_program: Incomplete | None = None, parameters: Incomplete | None = None, no_grad_set: Incomplete | None = None): ...
    def step(self) -> None: ...
    def set_state_dict(self, state_dict) -> None: ...
    def __getattr__(self, item): ...

class DygraphShardingOptimizerV2:
    tensor_fusion: Incomplete
    comm_overlap: Incomplete
    pp_overlap: Incomplete
    def __init__(self, optimizer, hcg) -> None: ...
    def register_reduce_overlap_hook(self, use_comm) -> None: ...
    def clear_grad(self, set_to_zero: bool = True) -> None: ...
    def filter_parameters(self, parameter_list, hcg): ...
    def reduce_gradients(self, parameter_list, hcg) -> None: ...
    def minimize(self, loss, startup_program: Incomplete | None = None, parameters: Incomplete | None = None, no_grad_set: Incomplete | None = None) -> None: ...
    def step(self) -> None: ...
    def set_state_dict(self, state_dict) -> None: ...
    def __getattr__(self, item): ...
