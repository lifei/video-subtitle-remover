from _typeshed import Incomplete
from paddle.distributed.fleet.meta_optimizers.common import OP_ROLE_KEY as OP_ROLE_KEY, OpRole as OpRole, is_backward_op as is_backward_op, is_loss_grad_op as is_loss_grad_op, is_optimizer_op as is_optimizer_op
from paddle.framework import core as core
from paddle.utils import unique_name as unique_name

def check_broadcast(block) -> None: ...
def check_allreduce_sum(block, shard, sharding_ring_id, dp_ring_id: int = -1) -> None: ...
def get_valid_op_role(block, insert_idx): ...
def insert_sync_calc_op(block, insert_idx, calc_dep_vars) -> None: ...
def insert_sync_comm_op(block, insert_idx, ring_id, comm_dep_vars): ...
def insert_sync_comm_ops(block, insert_idx, ring_id, comm_dep_vars): ...
def insert_fill_constant_ops(block, insert_idx, fill_constant_vars) -> None: ...
def insert_cast_ops(block, insert_idx, cast_ops) -> None: ...
def insert_allreduce_ops(block, insert_idx, ring_id, allreduce_vars, op_role=..., use_calc_stream: bool = False, user_defined_strategy: Incomplete | None = None) -> None: ...

class FuseHelper:
    @staticmethod
    def sort_vars_by_dtype(block, vars_name): ...
    @staticmethod
    def get_fused_groups(block, vars_name, fuse_size: float = 32.0): ...
    @staticmethod
    def insert_coalesce_tensor(block, index, groups, op_role=..., prefix: str = 'Output'): ...

def insert_fused_allreduce_ops(block, insert_idx, ring_id, allreduce_vars, op_role=..., use_calc_stream: bool = False, fuse_grad_size_in_MB: int = 32) -> None: ...
def insert_fused_reduce_ops(block, insert_idx, ring_id, reduce_vars, shard, op_role=..., use_calc_stream: bool = False, rank: Incomplete | None = None, fuse_grad_size: int = 32): ...
def insert_reduce_ops(block, insert_idx, ring_id, reduce_vars, shard, op_role=..., use_calc_stream: bool = False, rank: Incomplete | None = None, strategy: Incomplete | None = None): ...
def insert_fused_broadcast_param_ops(block, insert_idx, ring_id, params, shard, op_role=..., use_calc_stream: bool = False, rank: Incomplete | None = None, fuse_size: int = 32): ...
def insert_broadcast_param_ops(block, insert_idx, ring_id, params, shard, op_role=..., use_calc_stream: bool = False, rank: Incomplete | None = None, strategy: Incomplete | None = None): ...
def fuse_opt_broadcast_param_ops(block, ring_id, shard, op_role=..., strategy: Incomplete | None = None) -> None: ...
def get_grad_device(grad_name, shard): ...
def get_first_check_finite_and_unscale_op_idx(block, raise_error: bool = True): ...
def get_first_optimize_op_idx(block): ...
def insert_broadcast_ops(block, insert_idx, ring_id, broadcast2root, use_calc_stream: bool = False) -> None: ...

DtypeToSize: Incomplete

def get_var_size(param): ...
def insert_scale_loss_grad_ops(block, scale: float = 1.0) -> None: ...
def comm_analyse(main_program): ...
def add_sync_comm(program, sharding_ring_id) -> None: ...
def save_persistables(exe, dirname, main_program, filename: Incomplete | None = None): ...
def append_naive_sync(block, sync_var, ring_id) -> None: ...
