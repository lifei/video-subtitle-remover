from _typeshed import Incomplete
from paddle.distributed.fleet.meta_optimizers.common import is_optimizer_op as is_optimizer_op
from paddle.distributed.fleet.meta_optimizers.sharding.fp16_helper import FP16Utils as FP16Utils
from paddle.distributed.fleet.meta_optimizers.sharding.utils import get_var_size as get_var_size

class Shard:
    global_params: Incomplete
    worker_idx: int
    worker_num: int
    global_param2device: Incomplete
    device2global_params: Incomplete
    def __init__(self) -> None: ...
    def setup(self, params_grads, worker_idx, worker_num) -> None: ...
    def has_param(self, var_name): ...
    def has_opt_var(self, var_name): ...
    def has_var(self, var_name): ...
    def find_broadcast_params(self, block): ...
    def device(self, var_name): ...
    def is_param(self, var_name): ...
    def is_opti_var(self, var_name): ...
    def filter_grads(self, grads): ...

class ProgramSegment:
    def __init__(self, block) -> None: ...
