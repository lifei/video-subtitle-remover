from paddle.distributed.fleet.meta_optimizers.common import OP_ROLE_KEY as OP_ROLE_KEY, OpRole as OpRole, is_optimizer_op as is_optimizer_op
from paddle.framework import core as core

class FP16Utils:
    def __init__(self) -> None: ...
    @staticmethod
    def is_fp16_cast_op(block, op, params): ...
    @staticmethod
    def is_fp32_cast_op(block, op): ...
    @staticmethod
    def remove_cast_op(block, params, segment, offset): ...
    @staticmethod
    def prune_fp16(block, shard, reduced_grads_to_param, ring_ids) -> None: ...
    @staticmethod
    def sync_amp_check_nan_inf(block, ring_ids) -> None: ...
