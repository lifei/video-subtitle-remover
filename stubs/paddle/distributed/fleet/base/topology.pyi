from _typeshed import Incomplete

__all__ = ['CommunicateTopology', 'HybridCommunicateGroup']

class ParallelMode:
    DATA_PARALLEL: int
    TENSOR_PARALLEL: int
    PIPELINE_PARALLEL: int
    SHARDING_PARALLEL: int
    SEGMENT_PARALLEL: int

class CommunicateTopology:
    coordinate: Incomplete
    def __init__(self, hybrid_group_names=['data', 'pipe', 'sharding', 'sep', 'model'], dims=[1, 1, 1, 1, 1]) -> None: ...
    def get_hybrid_group_names(self): ...
    def get_dim(self, axis_name): ...
    def world_size(self): ...
    def get_rank(self, **args): ...
    def get_coord(self, rank): ...
    def get_axis_list(self, axis_name, index): ...
    def get_dim_size(self, axis_name): ...
    def get_fused_ranks(self, fused_axis): ...
    def get_comm_list(self, axis_name): ...
    def get_rank_from_stage(self, global_rank, **kwargs): ...

class HybridCommunicateGroup:
    nranks: Incomplete
    global_rank: Incomplete
    stage_id: Incomplete
    is_first_stage: Incomplete
    is_last_stage: Incomplete
    def __init__(self, topology) -> None: ...
    def get_parallel_mode(self): ...
    def topology(self): ...
    def get_global_rank(self): ...
    def get_data_parallel_rank(self): ...
    def get_data_parallel_world_size(self): ...
    def get_data_parallel_group(self): ...
    def get_data_parallel_group_src_rank(self): ...
    def get_model_parallel_rank(self): ...
    def get_model_parallel_world_size(self): ...
    def get_model_parallel_group(self): ...
    def get_model_parallel_group_src_rank(self): ...
    def get_stage_id(self): ...
    def get_pipe_parallel_world_size(self): ...
    def get_sep_parallel_rank(self): ...
    def get_sep_parallel_world_size(self): ...
    def get_sep_parallel_group(self): ...
    def get_sep_parallel_group_src_rank(self): ...
    def get_pipe_parallel_group(self): ...
    def get_p2p_groups(self): ...
    def get_sharding_parallel_rank(self): ...
    def get_sharding_parallel_world_size(self): ...
    def get_sharding_parallel_group(self): ...
    def get_sharding_parallel_group_src_rank(self): ...
    def get_check_parallel_group(self, sharding: bool = False): ...
    def get_rank_from_stage(self, stage_id, **kwargs): ...
    def get_dp_sep_parallel_group(self): ...
    def get_pp_mp_parallel_group(self): ...
    def create_fuse_group(self, fused_strategy_list): ...

class _CommunicateGroup:
    groups: Incomplete
    def __init__(self) -> None: ...
    def set_comm_group(self, group_name, group_rank, group_size, ring_id, group_ranks) -> None: ...
    def get_group(self, group_name): ...
    def get_model_parallel_group(self): ...
    def get_model_parallel_world_size(self): ...
    def get_model_parallel_rank(self): ...
