from _typeshed import Incomplete
from enum import Enum
from paddle.base import core as core
from paddle.base.framework import Operator as Operator, Parameter as Parameter, Program as Program, get_flags as get_flags
from paddle.distributed.auto_parallel.static.utils import get_logger as get_logger, is_backward_op as is_backward_op, is_forward_op as is_forward_op, is_optimize_op as is_optimize_op, use_new_executor as use_new_executor
from paddle.distributed.fleet.meta_optimizers.common import OpRole as OpRole

__not_shape_var_type__: Incomplete
logger: Incomplete

class AutoParallelStreamType(Enum):
    CALC_STREAM = 'default'
    MP_STREAM = 'auto_parallel_mp'
    SHARDING_STREAM = 'auto_parallel_sharding'

def list_to_ordered_dict(list_obj, ordered_dict: Incomplete | None = None): ...
def get_inputs_of_program(program): ...
def get_outputs_of_program(program): ...
def prune_program(program, start_op_idx, end_op_idx): ...
def split_program(program, op_indices): ...

class OpInOutInfo:
    def __init__(self) -> None: ...
    @property
    def is_build(self): ...
    def build_info(self, op) -> None: ...
    def is_needed(self, arg_name): ...

def var_can_be_deleted(var_name, block): ...
def prepare_ir_program(cur_prog, next_prog) -> None: ...
def set_skip_gc_vars(num_micro_batches, job_types, sub_programs, jobs): ...
