from ..utils import get_logger as get_logger
from .algorithms import new_algorithm as new_algorithm
from .config import TuningConfig as TuningConfig
from .trial import TrialStatus as TrialStatus
from paddle.distributed.auto_parallel.static.completion import Completer as Completer
from paddle.distributed.auto_parallel.static.dist_context import DistributedContext as DistributedContext
from paddle.distributed.auto_parallel.static.partitioner import Partitioner as Partitioner
from paddle.distributed.auto_parallel.static.process_group import clear_all_process_groups as clear_all_process_groups, get_all_process_groups as get_all_process_groups, new_process_group as new_process_group
from paddle.distributed.auto_parallel.static.reshard import Resharder as Resharder
from paddle.distributed.auto_parallel.static.utils import debug_program as debug_program
from paddle.distributed.passes import PassContext as PassContext, new_pass as new_pass
from paddle.static import append_backward as append_backward, program_guard as program_guard
from paddle.utils import unique_name as unique_name

def parse_process_groups(): ...
def get_metric(results): ...
def parse_results(results): ...

class OptimizationTuner:
    def __init__(self, dist_context, dataset, inputs_spec, labels_spec, batch_size, rank) -> None: ...
    @property
    def project_dir(self): ...
    @property
    def rank(self): ...
    @property
    def device_id(self): ...
    def get_best_config(self): ...
    def summary(self) -> None: ...
    def clear(self) -> None: ...
    def tune(self) -> None: ...
