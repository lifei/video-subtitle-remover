import abc
from ..dist_attribute import OperatorDistAttr as OperatorDistAttr
from ..process_group import new_process_group as new_process_group
from ..utils import compute_compatible_dims_mapping as compute_compatible_dims_mapping, is_optimize_op as is_optimize_op, set_dist_op_desc_original_id as set_dist_op_desc_original_id
from _typeshed import Incomplete
from paddle.base.log_helper import get_logger as get_logger
from paddle.distributed.fleet.meta_optimizers.common import OP_ROLE_KEY as OP_ROLE_KEY, OpRole as OpRole

BACKWARD_ONLY_DIST_OPS: Incomplete

class ParallelMode:
    DataParallel: str
    TensorParallel: str
    PipelineParalel: str
    MoEParallel: str

class SyncMode:
    AmpFlagSync: str
    GlobalNormSync: str

def is_elementwise_op(op_type): ...

class DistributedOperatorImplContainer(abc.ABC):
    def __init__(self, op_type) -> None: ...
    @property
    def type(self): ...
    @type.setter
    def type(self, op_type) -> None: ...
    @property
    def impls(self): ...
    def register_impl(self, dist_impl) -> None: ...
    def get_impl(self, impl_idx): ...
    def get_input_compatible_impls(self, dist_op): ...
    def get_output_compatible_impls(self, dist_op): ...
    def get_compatible_impls(self, dist_op): ...
    def update_dims_mapping(self, dist_op) -> None: ...
    def mapping_to_dist_operator_impl(dist_op, original_op_dist_attr) -> None: ...

class DistributedOperatorImpl(abc.ABC, metaclass=abc.ABCMeta):
    def __init__(self, name) -> None: ...
    @property
    def name(self): ...
    @name.setter
    def name(self, name) -> None: ...
    @property
    def type(self): ...
    @type.setter
    def type(self, op_type) -> None: ...
    @property
    def idx(self): ...
    @idx.setter
    def idx(self, impl_idx) -> None: ...
    @abc.abstractmethod
    def is_input_compatible(self, dist_op): ...
    @abc.abstractmethod
    def is_output_compatible(self, dist_op): ...
    @abc.abstractmethod
    def is_auto_compatible(self, dist_op): ...
    @staticmethod
    @abc.abstractmethod
    def forward(dist_ctx, *args, **kwargs): ...
    @staticmethod
    @abc.abstractmethod
    def backward(dist_ctx, *grad_outputs, **kwargs): ...
    def update_dims_mapping(self, dist_op) -> None: ...

def register_distributed_operator_impl_container(container) -> None: ...
def get_distributed_operator_impl_container(op_type): ...
def register_distributed_operator_impl(op_type, dist_impl) -> None: ...
def find_compatible_distributed_operator_impls(dist_op, fwd: bool = True, partial: bool = True): ...
def find_distributed_operator_impl_container(dist_op): ...
def is_parameter_related(varname, block, dist_context: Incomplete | None = None): ...
def infer_shape(block, src_var, src_var_dist_attr, op_input_dist_attr): ...
def set_comm_op_dist_attr_for_program(new_op, process_mesh, tensor_dist_attr, ctx, **kwargs) -> None: ...
def naive_copy_op_dist_attr_for_program(new_op, ref_op, ctx) -> None: ...
def get_data_parallel_group(dist_ctx, op, act_grad_names, rank): ...
def sync_and_scale_gradients(dist_ctx, op, groups, allreduce_var_names) -> None: ...
def get_partial_groups(dist_ctx, op, out_grad_names, rank): ...
def gradient_synchronization(dist_ctx, op, act_grad_names, out_grad_names, rank) -> None: ...
def is_data_parallel_scale_op(op): ...
def is_data_parallel_reduce_op(op): ...
def is_amp_flag_sync_op(op): ...
def is_global_norm_sync_op(op): ...
def is_in_backward_phase(dist_ctx): ...
def merge_forward_backward_dims_mapping(fw_results, bw_results): ...
def update_op_dims_mapping(dist_op, input_arg_names, output_arg_names, fw_results, bw_results): ...
def get_default_distributed_operator_impl(): ...
def copy_op_without_infer_shape(src_op, block, ctx, varname_kwargs): ...
