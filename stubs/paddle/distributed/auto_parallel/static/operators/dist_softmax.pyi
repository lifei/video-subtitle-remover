from ..cost import SoftmaxGradOpCost as SoftmaxGradOpCost, SoftmaxOpCost as SoftmaxOpCost, build_comp_costs_from_descs as build_comp_costs_from_descs, build_comp_desc_from_dist_op as build_comp_desc_from_dist_op, build_dp_costs as build_dp_costs
from ..utils import compute_compatible_and_update_dim_mapping as compute_compatible_and_update_dim_mapping, is_dim_shard as is_dim_shard
from .common import DistributedOperatorImpl as DistributedOperatorImpl, DistributedOperatorImplContainer as DistributedOperatorImplContainer, is_parameter_related as is_parameter_related, register_distributed_operator_impl as register_distributed_operator_impl, register_distributed_operator_impl_container as register_distributed_operator_impl_container
from .dist_default import DistributedDefaultImpl0 as DistributedDefaultImpl0
from paddle.distributed.fleet.meta_optimizers.common import OpRole as OpRole

class DistributedSoftmax(DistributedOperatorImplContainer):
    def __init__(self, op_type) -> None: ...

class DistributedSoftmaxImpl(DistributedOperatorImpl):
    def __init__(self, name) -> None: ...
    def calc_cost(self, op_role, dist_op, ctx, cluster): ...
    def calc_fwd_cost(self, dist_op, ctx, cluster): ...
    def calc_bwd_cost(self, dist_op, ctx, cluster): ...
    def is_input_compatible(self, dist_op): ...
    def is_output_compatible(self, dist_op): ...
    def is_auto_compatible(self, dist_op): ...
    def update_dims_mapping(self, dist_op): ...
    @staticmethod
    def forward(ctx, *args, **kwargs) -> None: ...
    @staticmethod
    def backward(ctx, *args, **kwargs) -> None: ...
