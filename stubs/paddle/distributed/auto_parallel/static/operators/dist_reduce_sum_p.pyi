from ..completion import get_phi_spmd_rule as get_phi_spmd_rule
from ..dist_attribute import OperatorDistAttr as OperatorDistAttr
from ..process_group import new_process_group as new_process_group
from ..utils import get_dist_tensor_spec as get_dist_tensor_spec, is_dim_shard as is_dim_shard, set_dist_op_desc_original_id as set_dist_op_desc_original_id
from .common import DistributedOperatorImpl as DistributedOperatorImpl, DistributedOperatorImplContainer as DistributedOperatorImplContainer, get_default_distributed_operator_impl as get_default_distributed_operator_impl, register_distributed_operator_impl as register_distributed_operator_impl, register_distributed_operator_impl_container as register_distributed_operator_impl_container, update_op_dims_mapping as update_op_dims_mapping
from paddle.distributed.fleet.meta_optimizers.common import OP_ROLE_KEY as OP_ROLE_KEY, OpRole as OpRole

class DistributedReduceSum(DistributedOperatorImplContainer):
    def __init__(self, op_type) -> None: ...
    @staticmethod
    def update_dims_mapping(dist_op): ...
    @staticmethod
    def mapping_to_dist_operator_impl(dist_op, original_op_dist_attr): ...

class DistributedReduceSumPrimtive(DistributedOperatorImplContainer):
    def __init__(self, op_type) -> None: ...

class DistributedReduceSumPrimtiveImpl0(DistributedOperatorImpl):
    def __init__(self, name) -> None: ...
    def is_input_compatible(self, dist_op): ...
    def is_output_compatible(self, dist_op): ...
    def is_auto_compatible(self, dist_op): ...
    def update_dims_mapping(self, dist_op): ...
    @staticmethod
    def forward(ctx, *args, **kwargs) -> None: ...
    @staticmethod
    def backward(ctx, *args, **kwargs) -> None: ...
