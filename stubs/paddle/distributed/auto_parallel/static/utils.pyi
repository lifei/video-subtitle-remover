from ..process_mesh import ProcessMesh as ProcessMesh
from .dist_attribute import DistTensorSpec as DistTensorSpec, OperatorDistAttr as OperatorDistAttr, TensorDistAttr as TensorDistAttr
from _typeshed import Incomplete
from paddle.base.wrapped_decorator import wrap_decorator as wrap_decorator
from paddle.framework import core as core
from paddle.framework.io_utils import is_belong_to_optimizer as is_belong_to_optimizer, is_parameter as is_parameter
from paddle.static import Variable as Variable

OpRole: Incomplete
OP_ROLE_KEY: Incomplete
__no_shape_var_type__: Incomplete
__not_naive_data_parallel_op__: Incomplete

def get_logger(log_level, name: str = 'auto_parallel'): ...
def is_valid_list_index(list, index): ...
def is_dim_shard(mapping): ...
def is_dim_replicate(mapping): ...
def verify_dims_mapping(dims_mapping, process_mesh): ...
def convert_to_dims_mapping(shard_spec, process_mesh): ...
def convert_to_shard_spec(dims_mapping, process_mesh): ...
def verify_shard_spec(shard_spec, tensor_shape, process_mesh): ...
def compute_compatible_dim_mapping(dim_mappings): ...
def compute_compatible_dims_mapping(dims_mapping_list): ...
def compute_compatible_process_mesh(process_mesh_list): ...
def compute_compatible_and_update_dim_mapping(dims_mapping_list, index_list): ...
def append_distributed_attr_suffix(name): ...
def remove_distributed_attr_suffix(name): ...
def check_distributed_attr_for_program(program, dist_context: Incomplete | None = None): ...
def print_program_with_dist_attr(program, dist_context: Incomplete | None = None) -> None: ...
def make_data_unshard(dist_main_prog, dist_startup_prog, dist_context: Incomplete | None = None) -> None: ...
def save_distributed_checkpoint(program, checkpoint_path, dist_attr_path, addition_info: Incomplete | None = None, is_integrated: bool = False, dist_context: Incomplete | None = None) -> None: ...
def load_distributed_checkpoint(checkpoint_path, dist_attr_path): ...
def load_checkpoint_into_program(checkpoint_path, dist_attr_path, program, dist_context: Incomplete | None = None): ...
def load_parameter_into_program(param_dict, program) -> None: ...
def get_dist_attr(program, dist_context: Incomplete | None = None): ...
def merge_and_slice_parameter(dist_param_dict, pre_dist_attr, cur_dist_attr): ...
def is_forward_op(op): ...
def is_backward_op(op): ...
def is_optimize_op(op): ...
def is_lr_sched_op(op): ...
def is_loss_op(op): ...
def is_loss_grad_op(op): ...
def is_gradient_clip_op(op): ...
def is_reshard_op(op): ...
def is_prim_op(op): ...
def is_comm_op(op): ...
def get_loss_op(block): ...
def set_var_dist_attr(dist_context, var, dims_mapping, process_mesh, **kwargs): ...
def naive_set_dist_op_attr_for_program_by_mesh_and_mapping(new_op, process_mesh, ref_mapping, ctx, **kwargs) -> None: ...
def naive_set_dist_op_attr_for_program_by_mesh(new_op, process_mesh, ctx, **kwargs) -> None: ...
def update_op_dims_mapping_by_default_dist_impl(dist_op): ...
def update_op_dims_mapping_by_elementwise_like_dist_impl(dist_op): ...
def get_all_distributed_main_program(serial_program_info, dist_context, parallelizer): ...

class SerialProgramInfo:
    def __init__(self, train_program, satrtup_program, loss, optimizer, cluster: Incomplete | None = None) -> None: ...
    @property
    def train_program(self): ...
    @property
    def startup_program(self): ...
    @property
    def loss(self): ...
    @property
    def optimizer(self): ...
    @property
    def cluster(self): ...

def get_standalone_cost_data(distributed_programs): ...
def set_dist_op_desc_original_id(dist_op_desc, op_desc, dist_context) -> None: ...
def to_list(value): ...
def debug_program(program, path, name) -> None: ...
def ring_id_to_process_group(ring_id): ...
def find_higher_order_backward_op(program): ...
def get_var_numel(var): ...
def get_lr(optimizer): ...
def initialize_pg_in_full_mode(all_process_groups, cur_rank) -> None: ...
def is_recompute_op(op): ...
def is_recompute_exclude_op(op): ...
def set_recompute_segments(model, losses, strategy, program) -> None: ...
def get_input_split_info(cur_rank, var, dist_context): ...
def validate_opt(optimizer): ...
def set_data_parallel(x): ...
def is_naive_data_parallel(dist_context): ...
def insert_dependencies_for_two_ops(block, idx, prior_op, posterior_op, dist_context, is_recompute: bool = False, sync: bool = False, op_namescope: Incomplete | None = None): ...
def insert_dependencies_for_vars(block, idx, prior_vars, post_vars, dist_context, oprole, process_mesh: Incomplete | None = None, is_recompute: bool = False, sync: bool = False, op_namescope: Incomplete | None = None, use_nop: bool = False): ...
def is_dep_skip_op(op): ...

dygraph_guard: Incomplete

def use_new_executor(): ...
def is_sequential_run(): ...
def get_pp_stage(dist_context, rank): ...
def wrap_data_for_completion(dist_op, input_names: list, output_names: list, attr_names: list): ...
def get_dist_tensor_spec(dist_op, name, is_input: bool = True): ...
