from .cost import AllgatherOpCost as AllgatherOpCost, CommContext as CommContext, ConcatOpCost as ConcatOpCost, SendOpCost as SendOpCost, SliceOpCost as SliceOpCost, SplitOpCost as SplitOpCost, build_comm_desc as build_comm_desc
from .dist_attribute import TensorDistAttr as TensorDistAttr
from .dist_context import DistributedContext as DistributedContext
from .process_group import new_process_group as new_process_group
from .utils import is_gradient_clip_op as is_gradient_clip_op, is_optimize_op as is_optimize_op
from _typeshed import Incomplete
from paddle.distributed.fleet.meta_optimizers.common import OpRole as OpRole
from paddle.framework import LayerHelper as LayerHelper, OpProtoHolder as OpProtoHolder, Program as Program, core as core
from paddle.utils import unique_name as unique_name

def get_var_with_recursion(var_name, block, program): ...

class EndOpDesc:
    def __init__(self, vars) -> None: ...
    @property
    def vars(self): ...

class AllGatherOpDesc:
    def __init__(self, group, shape, is_bool: bool = False, need_split: bool = True) -> None: ...
    @property
    def is_bool(self): ...
    @property
    def group(self): ...
    @property
    def desc(self): ...
    @property
    def shape(self): ...
    @property
    def need_split(self): ...

class AllGatherConcatOpDesc:
    def __init__(self, group, shape, is_bool: bool = False) -> None: ...
    @property
    def is_bool(self): ...
    @property
    def group(self): ...
    @property
    def desc(self): ...
    @property
    def shape(self): ...

class SendOpDesc:
    def __init__(self, partition_index, src, dst, is_bool: bool = False) -> None: ...
    @property
    def src(self): ...
    @property
    def is_bool(self): ...
    @property
    def partition_index(self): ...
    @property
    def dst(self): ...
    @property
    def desc(self): ...
    @property
    def shape(self): ...

class RecvOpDesc:
    def __init__(self, partition_index, src, dst, is_bool: bool = False) -> None: ...
    @property
    def dst(self): ...
    @property
    def is_bool(self): ...
    @property
    def partition_index(self): ...
    @property
    def src(self): ...
    @property
    def desc(self): ...
    @property
    def shape(self): ...

class SliceOpDesc:
    def __init__(self, starts, ends, axes, shape: Incomplete | None = None) -> None: ...
    @property
    def starts(self): ...
    @property
    def ends(self): ...
    @property
    def axes(self): ...
    @property
    def desc(self): ...
    @property
    def shape(self): ...

class ConcatOpDesc:
    def __init__(self, partition_index_list) -> None: ...
    @property
    def partition_index_list(self): ...
    @property
    def desc(self): ...

class Inserter:
    @staticmethod
    def insert_cast_op(block, idx, tensor, op_role, tensor_type): ...
    @staticmethod
    def insert_send_op(block, idx, tensor, src, dst, op_role) -> None: ...
    @staticmethod
    def insert_recv_op(block, idx, tensor, src, dst, op_role) -> None: ...
    @staticmethod
    def insert_reset_lod_op(block, idx, X, Y, op_role): ...
    @staticmethod
    def insert_concat_op(block, idx, tensors, axis, op_role): ...
    @staticmethod
    def insert_slice_op(block, idx, tensor, starts, ends, axes, new_var_name, op_role): ...
    @staticmethod
    def insert_split_op(block, idx, tensor, num_or_sections, op_role, axis: int = 0): ...
    @staticmethod
    def insert_fill_constant_op(block, idx, op_role, shape): ...
    @staticmethod
    def insert_allgather_op(block, idx, tensor, ranks, op_role, need_split): ...
    @staticmethod
    def insert_c_concat_op(block, idx, tensor, ranks, op_role): ...
    @staticmethod
    def concat_partitions_with_op(partition_tensor_list, tensor, partition_index, block, idx, op_role) -> None: ...

class Remover:
    @staticmethod
    def remove_no_need_ops(auto_parallel_main_prog, dist_context, rank_id) -> None: ...
    @staticmethod
    def remove_no_need_vars(auto_parallel_main_prog, dist_params_grads, feed_var_names) -> None: ...
    @staticmethod
    def remove_no_need_in_main(auto_parallel_main_prog, dist_context, rank_id, dist_params_grads) -> None: ...
    @staticmethod
    def remove_no_need_in_startup(auto_parallel_main_prog, auto_parallel_startup_prog) -> None: ...

class Resharder:
    while_block_info: Incomplete
    def __init__(self, auto_parallel_main_prog, auto_parallel_startup_prog, rank_id, dist_context, dist_params_grads, batch_size: Incomplete | None = None) -> None: ...
    @property
    def auto_parallel_main_prog(self): ...
    @property
    def auto_parallel_startup_prog(self): ...
    @property
    def rank_id(self): ...
    @property
    def dist_context(self): ...
    @property
    def dist_params_grads(self): ...
    @property
    def batch_size(self): ...
    @property
    def has_sent(self): ...
    @property
    def has_recv(self): ...
    @property
    def has_allgather(self): ...
    @staticmethod
    def compute_partition_shape(complete_shape, dims_mapping, process_shape): ...
    @staticmethod
    def compute_process_index(process, process_group, process_shape): ...
    @staticmethod
    def compute_partition_index(process, complete_shape, dims_mapping, process_shape, process_group): ...
    @staticmethod
    def compute_concat_info(partition_index_x, partition_index_y): ...
    @staticmethod
    def compute_complete_shape(slice_shape, process_shape, dims_mapping): ...
    @staticmethod
    def concat_partitions(partition_index_list, partition_index) -> None: ...
    @staticmethod
    def change_while_op_input_and_output(auto_parallel_main_prog, dist_context) -> None: ...
    def is_overlapped(self, shape_x, shape_y): ...
    def is_unshard(self, dims_mapping): ...
    def is_special_op(self, op): ...
    def is_condition_replicative(self, op): ...
    def need_reshard(self, dist_tensor, dist_attr, op_input: bool = True, dist_op: Incomplete | None = None): ...
    def get_op_process_meshes(self, op): ...
    def find_op_desc_seq(self, dist_tensor, dist_attr, serial: bool = False): ...
    def parse_op_desc(self, block, op_desc_seq, var_name, reshard_op, dist_attr): ...
    def get_op_input_attrs(self, op, var_name): ...
    def reshard(self) -> None: ...
    def get_cost(self, op, tensor, cluster): ...
    def parse_op_desc_for_cost(self, reshard_op_desc, dtype, cluster): ...
