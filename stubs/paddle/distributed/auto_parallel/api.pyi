import paddle
import paddle.distributed as dist
from .placement_type import check_placements_equal as check_placements_equal, get_shard_spec as get_shard_spec
from _typeshed import Incomplete
from paddle import nn as nn
from paddle.base import unique_name as unique_name
from paddle.base.framework import EagerParamBase as EagerParamBase, Variable as Variable, default_main_program as default_main_program
from paddle.distributed.auto_parallel import Engine as Engine, strategy as auto_strategy
from paddle.distributed.auto_parallel.placement_type import to_placements as to_placements
from paddle.distributed.auto_parallel.static.completion import mark_as_sharding_propagation_skip_op as mark_as_sharding_propagation_skip_op
from paddle.distributed.auto_parallel.static.dist_context import get_default_distributed_context as get_default_distributed_context
from paddle.distributed.auto_parallel.static.dist_op import DistributedOperator as DistributedOperator
from paddle.distributed.auto_parallel.static.utils import convert_to_dims_mapping as convert_to_dims_mapping, get_dist_attr as get_dist_attr
from paddle.framework import core as core
from typing import Callable

class DistAttr(core.TensorDistAttr):
    process_mesh: Incomplete
    dims_mapping: Incomplete
    def __init__(self, mesh, sharding_specs) -> None: ...
    @property
    def sharding_specs(self): ...

class DistModel:
    dist_loader: Incomplete
    def __init__(self, layer, loader, loss: Incomplete | None = None, optimizer: Incomplete | None = None, strategy: Incomplete | None = None, metrics: Incomplete | None = None) -> None: ...
    def train(self) -> None: ...
    def eval(self) -> None: ...
    def predict(self) -> None: ...
    def dist_main_program(self, mode: Incomplete | None = None): ...
    def dist_startup_program(self, mode: Incomplete | None = None): ...
    def serial_main_program(self, mode: Incomplete | None = None): ...
    def serial_startup_program(self, mode: Incomplete | None = None): ...
    def __call__(self, *args): ...
    def state_dict(self, mode: str = 'all'): ...
    def set_state_dict(self, state_dict) -> None: ...

class FusePasses:
    enable: bool
    gemm_epilogue: bool
    dropout_add: bool
    def __init__(self, config_dict: Incomplete | None = None) -> None: ...

class Strategy(auto_strategy.BaseConfig):
    def __init__(self, config: Incomplete | None = None) -> None: ...
    @property
    def sharding(self): ...
    @property
    def gradient_merge(self): ...
    @property
    def fused_passes(self): ...
    @property
    def pipeline(self): ...

def to_static(layer: paddle.nn.Layer, loader: Incomplete | None = None, loss: Incomplete | None = None, optimizer: Incomplete | None = None, strategy: Incomplete | None = None): ...
def shard_tensor(data, mesh, placements, dtype: Incomplete | None = None, place: Incomplete | None = None, stop_gradient: bool = True): ...
def dtensor_from_fn(fn, mesh, placements, *args, **kwargs): ...
def reshard(dist_tensor, mesh, placements): ...
def shard_layer(layer: nn.Layer, process_mesh: dist.ProcessMesh, shard_fn: Callable = None, input_fn: Callable = None, output_fn: Callable = None) -> nn.Layer: ...

class _ShardOptimizer:
    target_block: Incomplete
    def __init__(self, optimizer, shard_fn: Incomplete | None = None) -> None: ...
    def step(self): ...
    def state_dict(self): ...
    def __getattr__(self, item): ...

def shard_optimizer(optimizer, shard_fn: Incomplete | None = None): ...
