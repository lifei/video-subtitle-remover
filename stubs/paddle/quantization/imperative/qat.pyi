from . import fuse_utils as fuse_utils, utils as utils
from ...static.quantization.quantization_pass import QuantWeightPass as QuantWeightPass, ReplaceFakeQuantDequantPass as ReplaceFakeQuantDequantPass
from ...static.quantization.utils import move_persistable_var_to_global_block as move_persistable_var_to_global_block
from _typeshed import Incomplete
from paddle.base.framework import IrGraph as IrGraph
from paddle.framework import core as core
from paddle.nn.quant import quant_layers as quant_layers

INFER_MODEL_SUFFIX: str
INFER_PARAMS_SUFFIX: str

def lazy_import_fleet(layer_name_map, fake_quant_input_layers): ...

class ImperativeQuantAware:
    fuse_conv_bn: Incomplete
    def __init__(self, quantizable_layer_type=['Conv2D', 'Linear', 'Conv2DTranspose', 'ColumnParallelLinear', 'RowParallelLinear'], weight_quantize_type: str = 'abs_max', activation_quantize_type: str = 'moving_average_abs_max', weight_bits: int = 8, activation_bits: int = 8, moving_rate: float = 0.9, fuse_conv_bn: bool = False, weight_preprocess_layer: Incomplete | None = None, act_preprocess_layer: Incomplete | None = None, weight_quantize_layer: Incomplete | None = None, act_quantize_layer: Incomplete | None = None, onnx_format: bool = False) -> None: ...
    def quantize(self, model): ...
    def save_quantized_model(self, layer, path, input_spec: Incomplete | None = None, **config) -> None: ...

class ImperativeQuantizeInputs:
    def __init__(self, quantizable_layer_type=['Conv2D', 'Linear', 'Conv2DTranspose'], weight_quantize_type: str = 'abs_max', activation_quantize_type: str = 'moving_average_abs_max', weight_bits: int = 8, activation_bits: int = 8, moving_rate: float = 0.9, weight_preprocess_layer: Incomplete | None = None, act_preprocess_layer: Incomplete | None = None, weight_quantize_layer: Incomplete | None = None, act_quantize_layer: Incomplete | None = None) -> None: ...
    def apply(self, model) -> None: ...

class ImperativeQuantizeOutputs:
    def __init__(self, moving_rate: float = 0.9, activation_bits: int = 8, onnx_format: bool = False) -> None: ...
    def apply(self, model) -> None: ...
    def save_quantized_model(self, model, path, input_spec: Incomplete | None = None, **config) -> None: ...
